# WhisperTRT

This project optimizes [OpenAI Whisper](https://github.com/openai/whisper) with [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt#:~:text=NVIDIA%20TensorRT%2DLLM%20is%20an,on%20the%20NVIDIA%20AI%20platform.).

When executing the ``base.en`` model on NVIDIA Jetson Orin Nano, WhisperTRT runs **~3x faster** while consuming only **~60%** the memory compared with PyTorch.  

WhisperTRT roughly mimics the API of the original Whisper model, making it easy to use.  

Check out the [performance](#performance) and [usage](#usage) details below!

## Installation

### Prerequisites

WhisperTRT is designed for NVIDIA Jetson devices with JetPack installed. Ensure you have:
- NVIDIA Jetson device (Orin Nano, Orin NX, AGX Orin, etc.)
- JetPack 5.0+ or 6.0+ installed
- Python 3.8+

### System Dependencies
```bash
sudo apt-get update
sudo apt-get install -y ffmpeg libsndfile1 build-essential
```

### Python Dependencies

**Important:** Dependencies must be installed in the correct order. WhisperTRT requires `openai-whisper` and other packages to be installed first.
```bash
# 1. Install core dependencies
pip3 install numpy psutil torch torchaudio

# 2. Install Whisper and audio processing libraries
pip3 install openai-whisper tiktoken librosa soundfile ffmpeg-python

# 3. Install WhisperTRT
cd /path/to/whisper_trt
pip3 install -e .
```

### Using requirements.txt

If you have a `requirements.txt` file:
```bash
# Install dependencies first
pip3 install -r requirements.txt

# Then install WhisperTRT
pip3 install -e .
```

**Sample requirements.txt:**
```txt
numpy>=1.24.0
psutil>=5.9.0
torch>=2.0.0
torchaudio>=2.0.0
openai-whisper>=20230314
tiktoken>=0.3.0
librosa>=0.10.0
soundfile>=0.12.0
ffmpeg-python>=0.2.0
```

### Verification

After installation, verify WhisperTRT is working:
```bash
python3 -c "from whisper_trt import load_trt_model; print('WhisperTRT installed successfully!')"
```

### Troubleshooting

**ModuleNotFoundError: No module named 'whisper' or 'psutil'**
- Install dependencies before WhisperTRT: `pip3 install openai-whisper psutil`

**TensorRT engine build fails**
- Ensure JetPack is properly installed with CUDA and TensorRT support
- Check that `nvidia-smi` shows your GPU

**First run is slow**
- This is normal! The first time you load a model, TensorRT builds and caches the engine (2-5 minutes)
- Subsequent runs will be much faster using the cached engine at `~/.cache/whisper_trt/`

## Performance

All benchmarks are generated by calling ``profile_backends.py``,
processing a 20 second audio clip.

### Execution Time

Execution time in seconds to transcribe 20 seconds of speech on Jetson Orin Nano. See [profile_backends.py](profile_backends.py) for details.

|     | whisper | faster_whisper | whisper_trt |
|-------|---------|--------------------|--------|
| tiny.en | 1.74 sec | 0.85 sec | **0.64 sec** |
| base.en | 2.55 sec | Unavailable | **0.86 sec** |

### Memory Consumption

Memory consumption to transcribe 20 seconds of speech on Jetson Orin Nano. See [profile_backends.py](profile_backends.py) for details.

|     | whisper | faster_whisper | whisper_trt |
|-------|---------|--------------------|--------|
| tiny.en | 569 MB | **404 MB** | 488 MB |
| base.en | 666 MB |  Unavailable | **439 MB** |

## Usage

### Python
```python3
from whisper_trt import load_trt_model

model = load_trt_model("tiny.en")

result = model.transcribe("speech.wav") # or pass numpy array

print(result['text'])
```

> You can download an example speech file from [here](https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav) or 
> ``wget https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav -O speech.wav``.

> You may want to save or load the model to a custom path.  To do so, simply initialize the model like this
> 
> ```python3
> model = load_trt_model("tiny.en", path="./my_folder/tiny_en_trt.pth")
> ```

### Supported Models

WhisperTRT supports the following models:
- `tiny.en`
- `base.en`
- `small.en`
- `medium.en`

> **Note:** Custom fine-tuned models are not directly supported. WhisperTRT requires models to be converted to TensorRT format, which is currently only available for official OpenAI Whisper models.

### Transcribe

This script simply runs the model once.  

> Please note:  The first time you call load_model, it takes some time to build the TensorRT engine (2-5 minutes).
> After the first run, the model will be cached in the directory ~/.cache/whisper_trt/.
```bash
python examples/transcribe.py tiny.en assets/speech.wav --backend whisper_trt
```

### Profile Backend

This scripts measures the latency and process memory when transcribing audio. It includes a warmup run for
more accurate timing.
```bash
python examples/profile_backend.py tiny.en assets/speech.wav --backend whisper_trt
```

Backend can be one of "whisper_trt", "whisper", or "faster_whisper".

### Live Transcription

This script demonstrates live transcription using a microphone and voice activity detection.
```bash
python examples/live_transcription.py tiny.en --backend whisper_trt
```

## Integration with Diarization

WhisperTRT can be combined with speaker diarization tools like [pyannote.audio](https://github.com/pyannote/pyannote-audio) for speaker-attributed transcriptions:
```bash
# Install pyannote for diarization
pip3 install pyannote.audio torch-audiomentations
```

See the `examples/` directory for integration examples.

## See also

- [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) - Used to convert PyTorch model to TensorRT and perform inference.
- [NanoLLM](https://github.com/dusty-nv/NanoLLM) - Large Language Models targeting NVIDIA Jetson.  Perfect for combining with ASR!
- [jetson-containers](https://github.com/dusty-nv/jetson-containers) - Pre-built Docker containers for Jetson with Whisper and other AI models
- [faster-whisper](https://github.com/guillaumekln/faster-whisper) - Alternative optimized Whisper implementation using CTranslate2